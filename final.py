# -*- coding: utf-8 -*-
"""Analysis2_adaboost12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QZbCFEMeM4FUGBDO6Yua3L_plNN_8GLn

**Problem Statement**

Twitter has now become a useful way to build one's business as it helps in giving the brand a voice and a personality. The platform is also a quick, easy and inexpensive way to gain valuable insight from the desired audience. Identifying the sentiments about the product/brand can help the business take better actions.

You have with you evaluated tweets about multiple brands. The evaluators(random audience) were asked if the tweet expressed positive, negative, or no emotion towards a product/brand and labelled accordingly.

**Dataset Description**

This dataset contains around 7k tweet text with the sentiment label.

The file train.csv has 3 columns

tweet_id - Unique id for tweets. tweet - Tweet about the brand/product sentiment - 0: Negative, 1: Neutral, 2: Positive, 3: Can't Tell
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import nltk
import matplotlib.pyplot as plt

train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Sentiment Analysis Hackathon/Data/train.csv')
test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Sentiment Analysis Hackathon/Data/test.csv')

train["sentiment"].value_counts().plot(kind='bar')
plt.xlabel('Sentiments')
plt.ylabel('Count')
plt.show()

"""From the above graph we can see that the target variable sentiment is imbalanced."""

text = train["tweet"].tolist()+test["tweet"].tolist()

"""**Topic Modelling**"""

from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter
import operator
nltk.download('wordnet')

tokenizer = RegexpTokenizer(r'\w+')

en_stop = list(stopwords.words('english')) + ['mention','link','rt','quot']

Lema = WordNetLemmatizer()

Texts = []
Text = []
for i in text:
    i = str(i)
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)
    stopped_tokens = [i for i in tokens if not i in en_stop]
    lemmatized_tokens = [Lema.lemmatize(i) for i in stopped_tokens]
    Text.append(lemmatized_tokens)
    BoW_dict = dict(Counter(lemmatized_tokens))
    sorted_d = sorted(BoW_dict.items(), key=operator.itemgetter(1), reverse=True)
    Texts.append(sorted_d)
print(Texts) 
print("\nTop 10 words:\n", sorted_d[:10])

from collections import Counter
import operator
import numpy as np
import nltk
from textblob import TextBlob
import warnings
warnings.filterwarnings('ignore')  
nltk.download('averaged_perceptron_tagger')

#Extracting the top 10 words with their count
top_10 = sorted_d[:10]

#Storing only the top 10 words
top_words=[]
for x in top_10:
    top_words.append(x[0])


#Code begins

# Joining the list back to sentences
BoW_joined = " ".join(lemmatized_tokens)

# Converting the data to textblob
blob = TextBlob(BoW_joined)

# Print the first 10 tags
print("First 10 tags:\n" ,blob.tags[:10])

#Extracting the tags
tags = blob.tags

#Initialising an empty list
nouns = []

#Extracing the words with NN tags
for x in tags:
    if x[1]=="NN":
        nouns.append(x[0])
        

#Comparing the two lists to extract the common elements        
top_nouns=[x for x in nouns if x in top_words]
top_nouns  = list(set(top_nouns))

print("\nTop Nouns:", top_nouns)

from gensim import corpora, models
dictionary = corpora.Dictionary(Text)
print(dictionary)
print(dictionary.token2id)

corpus = [dictionary.doc2bow(text) for text in Text]
for line in corpus:
    print(line)

import gensim
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)

print(ldamodel)

for topic in ldamodel.print_topics(num_topics=3, num_words=3):
    print(topic)

"""**Back to Sentiment Analysis**"""



import nltk
import re
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from nltk.tokenize import TweetTokenizer

from nltk.corpus import stopwords
from string import punctuation
nltk.download('stopwords')
nltk.download('punkt')
stop_words = stopwords.words('english')
add_stopwords = ['ain','aren',"aren't",'couldn',"couldn't",'didn',"didn't",'doesn',"doesn't",'hadn',"hadn't",'mightn',"mightn't",'not']
for i in add_stopwords :
  stop_words.remove(i)
t = TweetTokenizer()

custom = stop_words+list(punctuation)+list('#sxsw')
from nltk.stem import WordNetLemmatizer,PorterStemmer
wordnet_lemmatizer = WordNetLemmatizer()
porter = PorterStemmer() 

nltk.download('wordnet')
#from nltk.stem.snowball import SnowballStemmer
#stemmer = SnowballStemmer('english')
import warnings
warnings.filterwarnings("ignore")
def my_tokenizer(s):
    s = str(s)
    s = s.lower()
    s=s.replace('___',"")
    #s=s.replace('#sxsw',"") 
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', s)
    rem_url=re.sub(r'http\S+', '',cleantext)
    rem_num = re.sub('[0-9]+', '', rem_url)
    tokens = t.tokenize(s)
    tokens = [t for t in tokens if len(t)>2] #remove words lesser than 2 in length
    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] #lemmatize words
    tokens = [porter.stem(t) for t in tokens] 
    tokens = [t for t in tokens if t not in custom] #remove stopwords and punctuation
    tokens = [t for t in tokens if not any(c.isdigit() for c in t)] # remove digits
    return tokens

"""Here we defined a funtion for data clening which first lower cases all the text because Now, NOW, now are considered as different word and it help in reducing the number of columns.
Now we're removing puntuation, stopwords, words having length less than 2 characters, digits and lemmatizing words.
"""

final_text = []
for x in text:
    final_text.append(my_tokenizer(x))

final_text = [' '.join(x) for x in final_text]

"""Now we can use this data for vectorizing."""

df = pd.DataFrame(final_text, columns = ["text"])

train_new = pd.DataFrame(df[:7274], columns = ['text'])
train_new['sentiment'] = train['sentiment']

train_new['sentiment'].value_counts()

import matplotlib.pyplot as plt

from wordcloud import WordCloud
wordcloud_0 = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(str(train_new[train_new['sentiment'] == 0]['text']))

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud_0, interpolation="bilinear")
plt.axis('off')
plt.show()

"""Word Cloud showing frequency for most frequent words for sentiment 0 ie Negative"""

wordcloud_1 = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(str(train_new[train_new['sentiment'] == 1]['text']))

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud_1, interpolation="bilinear")
plt.axis('off')
plt.show()

"""Word Cloud showing frequency for most frequent words for sentiment 1 ie Nutral"""

wordcloud_2 = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(str(train_new[train_new['sentiment'] == 2]['text']))

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud_2, interpolation="bilinear")
plt.axis('off')
plt.show()

"""Word Cloud showing frequency for most frequent words for sentiment 2 ie Positive"""

wordcloud_3 = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(str(train_new[train_new['sentiment'] == 3]['text']))

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud_3, interpolation="bilinear")
plt.axis('off')
plt.show()

"""Word Cloud showing frequency for most frequent words for sentiment 3 ie Cant Tell"""

X = df['text']

"""To models we cannot feed text data, therefore we've to convert this text data to vectors. For doing so there are different ways possible such as count vectorizer, TFIDF vectorizer, etc."""

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(stop_words="english")

X = tfidf.fit_transform(X)

X_array = X.toarray()

X_array.shape

ada_train = X_array[:7274].tolist()
ada_test = X_array[7274:].tolist()

train_features = pd.DataFrame(X_array[:7274])
df_X_train = train_features[:]

df_X_train.shape

test_features = pd.DataFrame(X_array[7274:])
df_X_test = test_features[:]

df_X_test.shape

df_y_train = train['sentiment']

df = pd.DataFrame(df_X_train)
df['sentiment'] = df_y_train
X = df.drop(columns = ['sentiment'])
y = df['sentiment']

from sklearn.model_selection import train_test_split as tts
X_train, X_test, y_train, y_test = tts(X, y, test_size = 0.2, random_state = 42)

from sklearn.metrics import f1_score, classification_report
from sklearn.svm import LinearSVC

svm = LinearSVC(random_state = 42)
svm.fit(X_train, y_train)

y_pred = svm.predict(X_test)

print('Classification Report of LinearSVC :', classification_report(y_test, y_pred))

from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(class_weight = 'balanced')
log_reg.fit(X_train, y_train)

y_pred_log = log_reg.predict(X_test)

print('Classification Report of Logistic Regression :', classification_report(y_test, y_pred_log))

from sklearn.tree import DecisionTreeClassifier
decision_tree = DecisionTreeClassifier(max_depth = 100)
decision_tree.fit(X_train, y_train)

y_pred_deci = decision_tree.predict(X_test)

print('Classification Report of Decision Tree :', classification_report(y_test, y_pred_deci))

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators = 100, n_jobs = -1, class_weight = 'balanced_subsample', random_state = 42)
rfc.fit(X_train, y_train)

y_pred_rfc = rfc.predict(X_test)

print('Classification Report of Random Forest :', classification_report(y_test, y_pred_rfc))

from sklearn.ensemble import AdaBoostClassifier
ada_linear_svc = LinearSVC()
adaboost = AdaBoostClassifier(base_estimator = ada_linear_svc, n_estimators = 100, algorithm = 'SAMME', random_state = 42)
adaboost.fit(X_train, y_train)

y_pred_adaboost = adaboost.predict(X_test)

print('Classification Report of AdaBoost Classifier :\n\n', classification_report(y_test, y_pred_adaboost))



ada_linear_svc_whole = LinearSVC()
adaboost_whole = AdaBoostClassifier(base_estimator = ada_linear_svc_whole, n_estimators = 100, algorithm = 'SAMME', random_state = 42)
adaboost_whole.fit(ada_train, train['sentiment'])

y_pred_adaboost_whole = adaboost_whole.predict(ada_test)

submission = pd.DataFrame({
        "tweet_id": test['tweet_id'],
        "sentiment": y_pred_adaboost_whole
    })

submission.to_csv('/content/drive/My Drive/Colab Notebooks/Sentiment Analysis Hackathon/submission.csv', index=False)

